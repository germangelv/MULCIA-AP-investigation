{"cells":[{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_v8IoPq6YgC","executionInfo":{"status":"ok","timestamp":1650910736999,"user_tz":-120,"elapsed":2906,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"335b8261-a142-4c72-8ee3-9346ccc4c4bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Estoy en Google CoLab\n","Requirement already satisfied: livelossplot in /usr/local/lib/python3.7/dist-packages (0.5.5)\n","Requirement already satisfied: numpy<1.22 in /usr/local/lib/python3.7/dist-packages (from livelossplot) (1.21.6)\n","Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n","Requirement already satisfied: ipython==7.* in /usr/local/lib/python3.7/dist-packages (from livelossplot) (7.32.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.18.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (3.0.29)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.4.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.1.3)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (57.4.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (5.1.1)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.6.1)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.8.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython==7.*->livelossplot) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython==7.*->livelossplot) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.5)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (4.2.0)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.8)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.4.2)\n"]}],"source":["import os\n","try:\n","    from google.colab import drive\n","    COLAB = True\n","    print(\"Estoy en Google CoLab\")\n","    %tensorflow_version 2.x\n","    !pip install livelossplot\n","except:\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n","    print(\"No estoy en Google CoLab\")\n","    COLAB = False"]},{"cell_type":"markdown","metadata":{"id":"hDTm2IzF6YgL"},"source":["# Autoencoders"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"j34xaIMB6YgP","executionInfo":{"status":"ok","timestamp":1650910737000,"user_tz":-120,"elapsed":36,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# Cargamos las librerías necesarias para la práctica\n","import keras\n","from keras import models\n","from keras import backend as K\n","from keras.models import Model\n","from keras.layers import Input, Conv2D, Dense, Flatten, LeakyReLU, Activation, Dropout\n","from keras.layers import Layer, Reshape, Conv2DTranspose, Lambda, ReLU, BatchNormalization\n","from keras.metrics import binary_crossentropy\n","from tensorflow.keras.optimizers import Adam\n","from keras.callbacks import ReduceLROnPlateau\n","try:\n","  from livelossplot import PlotLossesKerasTF\n","except:\n","  !pip install livelossplot\n","  from livelossplot import PlotLossesKerasTF\n","from keras.utils.vis_utils import plot_model\n","from keras.datasets import mnist, cifar10\n","from tqdm import tqdm\n","from io import BytesIO, StringIO\n","import inspect\n","import numpy as np\n","import pathlib\n","import cv2\n","import matplotlib\n","import os\n","import random\n","import re\n","import tensorflow as tf\n","# %matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","matplotlib.rcParams['figure.figsize'] = (15,5) # Para el tamaño de la imagen\n","matplotlib.rcParams['figure.figsize'] = (15,5)\n","\n","import seaborn as sns\n","import pandas as pd\n","# import utils"]},{"cell_type":"markdown","metadata":{"id":"1khVxSqR6YgR"},"source":["## Introducción\n","\n","Hasta el momento se han visto aplicaciones de aprendizaje supervisado con redes neuronales (convolucionales y recurrentes). Esto es, para cada muestra en nuestro conjunto de datos teníamos asociada una etiqueta o respuesta esperada.\n","\n","En este tema vamos a trabajar en aplicaciones de aprendizaje no supervisado. Ahora no tenemos la etiqueta asociada a cada muestra, lo cuál suele ser muy costoso de conseguir. La objetivo principal del aprendizaje no supervisado es evitar el uso de muestra equitadas o al menos ayudar a reducir el tamaño de las muestras etiquetas.\n","\n","La estructura en deep learning por excelencia para aprendizaje no supervisado es el **autoencoder**. Esta red, fue diseñada para buscar nuevas representaciones de las entradas. Para ello se divide la red en dos partes, el codificador y el decodificador. El **codificador** llevará a cabo una compresión de la entrada hasta generar un vector (llamado **vector latente**), representación de nuestra entrada. El **decodificador**, tendrá que usar este vector latente para *reconstruir* la imagen original de entrada.\n","\n","<img src='https://hdvirtual.us.es/discovirt/index.php/s/YDRYPdrLzAwCS6Q/preview' width=70% />\n","\n","El vector latente de es menor dimensionalidad que la entrada. Es decir, si estamos trabajando con imágenes de 200x200 el vector debe ser de una dimensión inferior a 40000 elementos. Esta reducción de dimensionalidad obliga al codificador a quedarse con la información más importante y representativa que permita al decodificador a llevar una reconstrucción lo más fiel posible.\n","\n","En cierto modo, tenemos dos redes independientes (el codificador y el decodificador) que trabajan para alcanzar un bien común. El el siguiente tema, sobre redes generativas adversarias, veremos que tambien se sigue este enfoque de varias redes colaborando para llevar a cabo una tarea objetivo. \n","\n","\n","## Nuestro primer autoencoder en keras\n","\n","Vamos a crear nuestro primer autoencoder en keras. Para ello, usaremos el conjunto de datos MNIST. Vamos a entrenar nuestro autoencoder para que aprenda una representación de cada dígito en un espacio bidimensional y después visualizaremos las representaciones aprendidas en un gráfico 2D.\n","\n","Puesto que los autoencoders tiene una estructura bien definida, nos podemos crear una clase para que nos facilite su creación:\n","\n","```\n","AE = Autoencoder(\n","    input_dim = (28,28,1)\n","    , encoder_conv_filters = [32,64,64,64]\n","    , encoder_conv_kernel_size = [3,3,3,3]\n","    , encoder_conv_strides = [1,2,2,1]\n","    , decoder_conv_filters = [64,64,32,1]\n","    , decoder_conv_kernel_size = [3,3,3,3]\n","    , decoder_conv_strides = [1,2,2,1]\n","    , z_dim = 2)\n","```\n","\n","\n","Vamos a empezar agregando el código de creación del codificador:\n"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"x96LfXZ96YgU","executionInfo":{"status":"ok","timestamp":1650910737003,"user_tz":-120,"elapsed":37,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["class Autoencoder(tf.keras.Model):\n","    \n","    def __init__(self, input_dim, encoder_conv_filters, encoder_conv_kernel_size, encoder_conv_strides,\n","                 decoder_conv_filters, decoder_conv_kernel_size, decoder_conv_strides, z_dim):\n","        super(Autoencoder, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.encoder_conv_filters = encoder_conv_filters\n","        self.encoder_conv_kernel_size = encoder_conv_kernel_size\n","        self.encoder_conv_strides = encoder_conv_strides\n","        self.decoder_conv_filters = decoder_conv_filters\n","        self.decoder_conv_kernel_size = decoder_conv_kernel_size\n","        self.decoder_conv_strides = decoder_conv_strides\n","        self.z_dim = z_dim\n","        \n","        self.encoder = self.__create_encoder()\n","        \n","    def __create_encoder(self):\n","\n","        encoder_input = Input(shape=self.input_dim, name='encoder_input') #(1)\n","        x = encoder_input\n","        for i in range(len(self.encoder_conv_filters)):\n","            conv_layer = Conv2D(\n","                filters = self.encoder_conv_filters[i],\n","                kernel_size = self.encoder_conv_kernel_size[i],\n","                strides = self.encoder_conv_strides[i],\n","                padding = 'same',\n","                name = 'encoder_conv_' + str(i)\n","            )\n"," \n","            x = conv_layer(x) #(2)\n","            x = LeakyReLU()(x)\n","\n","        shape_before_flattening = K.int_shape(x)[1:]\n","        x = Flatten()(x) #(3)\n","        \n","        encoder_output= Dense(self.z_dim, name='encoder_output')(x) #(4)\n","        \n","        return Model(encoder_input, encoder_output) #(5)"]},{"cell_type":"markdown","metadata":{"id":"lkofdPPv6YgW"},"source":["   1. Definimos la entrada del autoencoder (en nuestro caso una imagen)\n","   2. Añadimos la capa convolucionales secuencialmente.\n","   3. Aplanamos la salida de la última capa convolucional para crear un vector.\n","   4. Añadimos una capa densa (totalmente conectada) para crear nuestro vector latente de 2 dimensiones.\n","   5. Creamos el modelo de Keras que define el codificador cuya entrada es la imagen y salida el vector latente.\n","    \n","Vamos a probarlo:"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HW7ZzT6w6YgX","executionInfo":{"status":"ok","timestamp":1650910737003,"user_tz":-120,"elapsed":35,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"64813c93-d12e-4233-87af-cc5948b9f413"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n","                                                                 \n"," encoder_conv_0 (Conv2D)     (None, 28, 28, 32)        320       \n","                                                                 \n"," leaky_re_lu_8 (LeakyReLU)   (None, 28, 28, 32)        0         \n","                                                                 \n"," encoder_conv_1 (Conv2D)     (None, 14, 14, 64)        18496     \n","                                                                 \n"," leaky_re_lu_9 (LeakyReLU)   (None, 14, 14, 64)        0         \n","                                                                 \n"," encoder_conv_2 (Conv2D)     (None, 7, 7, 64)          36928     \n","                                                                 \n"," leaky_re_lu_10 (LeakyReLU)  (None, 7, 7, 64)          0         \n","                                                                 \n"," encoder_conv_3 (Conv2D)     (None, 7, 7, 64)          36928     \n","                                                                 \n"," leaky_re_lu_11 (LeakyReLU)  (None, 7, 7, 64)          0         \n","                                                                 \n"," flatten_4 (Flatten)         (None, 3136)              0         \n","                                                                 \n"," encoder_output (Dense)      (None, 2)                 6274      \n","                                                                 \n","=================================================================\n","Total params: 98,946\n","Trainable params: 98,946\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["AE = Autoencoder(\n","    input_dim = (28,28,1)\n","    , encoder_conv_filters = [32,64,64,64]\n","    , encoder_conv_kernel_size = [3,3,3,3]\n","    , encoder_conv_strides = [1,2,2,1]\n","    , decoder_conv_filters = [64,64,32,1]\n","    , decoder_conv_kernel_size = [3,3,3,3]\n","    , decoder_conv_strides = [1,2,2,1]\n","    , z_dim = 2)\n","\n","AE.encoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"DdX2MdN_6YgZ"},"source":["Podemos cambiar el número de capas convolucionales simplemente añadiendo elementos a las listas. **Se recomienda experimentar con los parámetros para entender mejor como afecta la arquitectura al número de parámetros de cada capa, el rendimiento del modelo y los tiempos de entrenamiento.**\n","\n","Continuamos ahora con el decodificador. Éste suele ser un *espejo* del codificador excepto por el tipo de capas convolucionales. El decodificador tendrá que mapear el vector latente *z* a las dimensiones de una imagen. Para ello usaremos tambien capas convolucionales para obtener dicha imagen. La salida de este red deberá ser la entrada del codificador.\n","\n","Para ello debemos aplicar una operación de deconvolución o las traspuesta del aconvolución:\n","\n","<img src='https://hdvirtual.us.es/discovirt/index.php/s/5C3ZgtpxstrDjwa/preview' />\n","\n","En Keras tenemos la clase `Conv2DTranspose` que define la funcionalidad de esta capa.\n","\n","Aunque hemos dicho que el decodificador suele ser una imagen espejo del codificador, no tiene porqué ser así. El decoficador puede tener cualquier estructura pero siempre debe cumplir la condición de que su salida debe tener las misma dimensiones que la entrada del codificador. Esta condición es necesaria para poder definir la función de coste comparando pixel a pixel la entrada del codificador con la salida del decodificador.\n","\n","Vamos ahora a añadir el método de creación del decodificador a nuestra clase `Autoencoder`:"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"2t3dpHi06Yga","executionInfo":{"status":"ok","timestamp":1650910737004,"user_tz":-120,"elapsed":17,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["class Autoencoder(tf.keras.Model):\n","    \n","    def __init__(self, input_dim, encoder_conv_filters, encoder_conv_kernel_size, encoder_conv_strides,\n","                 decoder_conv_filters, decoder_conv_kernel_size, decoder_conv_strides, activation, z_dim,\n","                 use_batch_normalization=False, dropout = None):\n","        super(Autoencoder, self).__init__()\n","        \n","        self.input_dim = input_dim\n","        self.encoder_conv_filters = encoder_conv_filters\n","        self.encoder_conv_kernel_size = encoder_conv_kernel_size\n","        self.encoder_conv_strides = encoder_conv_strides\n","        self.decoder_conv_filters = decoder_conv_filters\n","        self.decoder_conv_kernel_size = decoder_conv_kernel_size\n","        self.decoder_conv_strides = decoder_conv_strides\n","        self.z_dim = z_dim\n","        self.activation = activation\n","        self.use_batch_normalization = use_batch_normalization\n","        self.dropout = dropout\n","        \n","        self.encoder, encoder_input, encoder_output = self.__create_encoder() \n","        self.decoder = self.__create_decoder()\n","        self.model = Model(encoder_input, self.decoder(encoder_output))\n","        \n","    def __create_encoder(self):\n","\n","        encoder_input = Input(shape=self.input_dim, name='encoder_input') \n","        x = encoder_input\n","        for i in range(len(self.encoder_conv_filters)):\n","            conv_layer = Conv2D(\n","                filters = self.encoder_conv_filters[i],\n","                kernel_size = self.encoder_conv_kernel_size[i],\n","                strides = self.encoder_conv_strides[i],\n","                padding = 'same',\n","                activation = self.activation if isinstance(self.activation, str) else None,\n","                name = 'encoder_conv_' + str(i)\n","            )\n"," \n","            x = conv_layer(x) #(2)\n","    \n","            if inspect.isclass(self.activation) and Layer in self.activation.__bases__:\n","                x = self.activation()(x)\n","                \n","            if self.use_batch_normalization:\n","                x = BatchNormalization()(x)\n","\n","            if self.dropout:\n","                x = Dropout(self.dropout)(x)                \n","\n","        self.__shape_before_flattening = K.int_shape(x)[1:]\n","        x = Flatten()(x) #(3)\n","        \n","        encoder_output = self._create_latent_vector(encoder_input, x)\n","        \n","        return Model(encoder_input, encoder_output), encoder_input, encoder_output #(5)\n","        \n","    def _create_latent_vector(self, encoder_input, x):\n","        encoder_output= Dense(self.z_dim, name='encoder_output')(x) \n","        return encoder_output\n","    \n","    \n","    def __create_decoder(self):\n","\n","        decoder_input = Input(shape=(self.z_dim,), name='decoder_input') #(1)\n","        \n","        x = Dense(np.prod(self.__shape_before_flattening))(decoder_input) #(2)\n","        \n","        \n","        x = Reshape(self.__shape_before_flattening)(x) #(3)\n","        \n","        for i in range(len(self.decoder_conv_filters)):\n","            activation = self.activation if isinstance(self.activation, str) else None\n","            conv_layer = Conv2DTranspose(\n","                filters = self.decoder_conv_filters[i],\n","                kernel_size = self.decoder_conv_kernel_size[i],\n","                strides = self.decoder_conv_strides[i],\n","                padding = 'same',\n","                activation = activation if i < len(self.decoder_conv_filters)-1 else 'sigmoid',\n","                name = 'decoder_conv_' + str(i)\n","            )\n"," \n","            x = conv_layer(x) #(4)\n","    \n","            if i < len(self.decoder_conv_filters)-1:\n","                if inspect.isclass(self.activation) and Layer in self.activation.__bases__:\n","                    x = self.activation()(x)\n","\n","                if self.use_batch_normalization:\n","                    x = BatchNormalization()(x)\n","\n","                if self.dropout:\n","                    x = Dropout(self.dropout)(x)                \n","            \n","            \n","        decoder_output = x\n","        \n","        return Model(decoder_input, decoder_output) #(6)\n","    "]},{"cell_type":"markdown","metadata":{"id":"qvTwESGR6Ygc"},"source":["   1. Definimos la entrada del decodificador (la entrada es el vector latente)\n","   2. Conectamos la entrada a una capa densa para expandir el vector.\n","   3. Debemos convertir el vector a una estructura 2D para que sirva de entrada a la primera capa convolucional.\n","   4. Vamos estacando la capas convolucionales.\n","   5. Debemos controla si es la última capa convolucional. En ese caso la activación debe ser una sigmoide.\n","   6. Creamos el modelo Keras que define el decodificador que toma como entrada el vector latente y como salida una imagen de las mismas dimensiones que la imagen de entrada.\n","    \n","Vamos a probarlo:"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Oz2DoO-6Ygd","executionInfo":{"status":"ok","timestamp":1650910737339,"user_tz":-120,"elapsed":350,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"62239a73-d478-48db-958d-b494fbe6e144"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," decoder_input (InputLayer)  [(None, 2)]               0         \n","                                                                 \n"," dense_2 (Dense)             (None, 3136)              9408      \n","                                                                 \n"," reshape_2 (Reshape)         (None, 7, 7, 64)          0         \n","                                                                 \n"," decoder_conv_0 (Conv2DTrans  (None, 7, 7, 64)         36928     \n"," pose)                                                           \n","                                                                 \n"," decoder_conv_1 (Conv2DTrans  (None, 14, 14, 64)       36928     \n"," pose)                                                           \n","                                                                 \n"," decoder_conv_2 (Conv2DTrans  (None, 28, 28, 32)       18464     \n"," pose)                                                           \n","                                                                 \n"," decoder_conv_3 (Conv2DTrans  (None, 28, 28, 1)        289       \n"," pose)                                                           \n","                                                                 \n","=================================================================\n","Total params: 102,017\n","Trainable params: 102,017\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["AE = Autoencoder(\n","    input_dim = (28,28,1)\n","    , encoder_conv_filters = [32,64,64,64]\n","    , encoder_conv_kernel_size = [3,3,3,3]\n","    , encoder_conv_strides = [1,2,2,1]\n","    , decoder_conv_filters = [64,64,32,1]\n","    , decoder_conv_kernel_size = [3,3,3,3]\n","    , decoder_conv_strides = [1,2,2,1]\n","    , activation = 'relu'\n","    , z_dim = 2)\n","\n","AE.decoder.summary()"]},{"cell_type":"markdown","metadata":{"id":"WqtGbKtU6Yge"},"source":["Para poder entrenar el autoencoder debemos unir el codificador y del decodificador. Para ello hemos modificador el método `__init__` en sus última líneas:\n","\n","```\n","        self.encoder, encoder_input, encoder_output = self.__create_encoder() #(1)\n","        self.decoder = self.__create_decoder()                                \n","        self.model = Model(encoder_input, self.decoder(encoder_output))       #(2) \n","```\n","\n","   1. La entrada del autoencoder será la misma que la entrada del codificador. Además la salida del codificador será la entrada decodificador. Hemos modificador el método `__create_encoder` para que nos devuelva esos tensores.\n","   2. Creamos el modelo Keras del autoencoder cuya entrada, como ya hemos dicho, es la entrada del codificador y su salida es la salida que dará el decodificador al darle como entrada al salida del codificador."]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYVFUXVk6Yge","executionInfo":{"status":"ok","timestamp":1650910737340,"user_tz":-120,"elapsed":25,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"aedc5f7c-7812-4dd6-dd48-5d3c1bf01bf4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_11\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoder_input (InputLayer)  [(None, 28, 28, 1)]       0         \n","                                                                 \n"," encoder_conv_0 (Conv2D)     (None, 28, 28, 32)        320       \n","                                                                 \n"," encoder_conv_1 (Conv2D)     (None, 14, 14, 64)        18496     \n","                                                                 \n"," encoder_conv_2 (Conv2D)     (None, 7, 7, 64)          36928     \n","                                                                 \n"," encoder_conv_3 (Conv2D)     (None, 7, 7, 64)          36928     \n","                                                                 \n"," flatten_5 (Flatten)         (None, 3136)              0         \n","                                                                 \n"," encoder_output (Dense)      (None, 2)                 6274      \n","                                                                 \n"," model_10 (Functional)       (None, 28, 28, 1)         102017    \n","                                                                 \n","=================================================================\n","Total params: 200,963\n","Trainable params: 200,963\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["AE.model.summary()"]},{"cell_type":"markdown","metadata":{"id":"zMoU9dRq6Ygf"},"source":["Ahora que ya tenemos nuestro modelo creado, podemos compilarlo usando una optimizador y una función de coste. Respecto a la función de coste tenemos varias opciones. Podemos usar RMSE, *binary cross entropy* o SSIM (similaridad estructural). La función de coste *binary cross entropy* realiza una comparación pixel a pixel entre la imagen original y la reconstruida penalizando mucho las malas predicciones, lo que hace que la red tienda a predecir el punto medio del rango. Esto da lugar a imágenes muy claras (con mucho gris). \n","\n","En nuestro caso usaremos RMSE como función de coste y SSIM como métrica. Es recomendable **probar binary cross entropy y SSIM como funciones de coste para ver su efecto**. "]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"id":"447TYNdR6Ygg","executionInfo":{"status":"error","timestamp":1650910917167,"user_tz":-120,"elapsed":388,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"67b0e3bd-ab26-44c4-a3de-81b6489279f2"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-7bfaad5a492f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mssim_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mssim_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"]}],"source":["ssim_loss = 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n","AE.model.compile(optimizer=Adam(), loss = 'mean_squared_error', metrics=[ssim_loss])"]},{"cell_type":"markdown","metadata":{"id":"gxtx4q_J6Ygg"},"source":["El último paso es entrenar nuestra autoencoder. Para ello necesitamos el conjunto de datos MNIST que viene con keras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGw4B3d76Ygh","executionInfo":{"status":"aborted","timestamp":1650910737341,"user_tz":-120,"elapsed":13,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# MNIST dataset\n","(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n","\n","image_size = x_train_mnist.shape[1]\n","x_train_mnist = np.reshape(x_train_mnist, [-1, image_size, image_size, 1])\n","x_test_mnist = np.reshape(x_test_mnist, [-1, image_size, image_size, 1])\n","x_train_mnist = x_train_mnist.astype('float32') / 255\n","x_test_mnist = x_test_mnist.astype('float32') / 255\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYMLtBpt6Ygh","executionInfo":{"status":"aborted","timestamp":1650910737342,"user_tz":-120,"elapsed":14,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["AE.model.fit(x=x_train_mnist, y=x_train_mnist, \n","             validation_data=(x_test_mnist, x_test_mnist),\n","             batch_size=128, shuffle=True, epochs=50,\n","             callbacks=[PlotLossesKerasTF()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQym4YII6Ygi","executionInfo":{"status":"aborted","timestamp":1650910738001,"user_tz":-120,"elapsed":673,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["AE.model.save_weights('AE_mnist_weights.h5')"]},{"cell_type":"markdown","metadata":{"id":"fJGRcUfZ6Ygi"},"source":["## Visualizando la reconstrucción\n","\n","Ahora que tenemos el autoencoder entrenado podemos comparar las imágenes de entrada y su reconstrucción:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XEcYKWxd6Ygj","executionInfo":{"status":"aborted","timestamp":1650910738003,"user_tz":-120,"elapsed":674,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["n = 25\n","imgs = [x_test_mnist[i:i+5] for i in range(0, 25, 5)]\n","draw2compare(*imgs, figsize=(10, 10))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jC1kA-td6Ygj","executionInfo":{"status":"aborted","timestamp":1650910738004,"user_tz":-120,"elapsed":674,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["n = 25\n","decoded_imgs = AE.model.predict(x_test_mnist[:n])\n","imgs = [decoded_imgs[i:i+5] for i in range(0, 25, 5)]\n","draw2compare(*imgs, figsize=(10, 10))"]},{"cell_type":"markdown","metadata":{"id":"xPRIp8tj6Ygj"},"source":["Aquí vemos algunas cosas interesantes:\n","   1. Algunos dígitis tienen reconstrucción. No es algo preocupante. Estamos usando una vector latente de 2D para llevar a cabo la visualización del espacio latente. **Se recomienda entrenar al autoencoder con un vector de mayor dimensión y ver la mejora en la reconstrucción**.\n","   2. Algunos dígitos se confunden con otros.\n","   3. En general la reconstrucciones están difuminadas.\n","   4. Podemos decir que el vector latente ha capturado ciertas características globales para poder reconstruir el dígito, pero no contiene información sobre detalles.\n","   \n","   \n"]},{"cell_type":"markdown","metadata":{"id":"sRiDrtFk6Ygk"},"source":["### Visualizando el espacio latente\n","\n","Para visualizar el espacio latente solo tenemos que pasar las muestras por el codificador y representarlas con un scatter plot. Como tenemos identificado que dígito es cada muestra, lo usaremos para identificar cada punto con un color:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"K5lcYFEi6Ygk","executionInfo":{"status":"aborted","timestamp":1650910738005,"user_tz":-120,"elapsed":22,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# función en utils.py\n","show_latent_space(AE, x_test_mnist)"]},{"cell_type":"markdown","metadata":{"id":"SCxdELnl6Ygk"},"source":["## Muestreando el espacio latente.\n","\n","Hemos generado un espacio dimensional inferior de representaciones (que naturalmente es un espacio vectorial) donde cada punto puede ser mapeado a imagen realista. Ahora podemos usar nuestro generador para crear nuevas muestras. Solo tenemos que pasar al decodificador nuevos vectores:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O3sfGAXM6Ygl","executionInfo":{"status":"aborted","timestamp":1650910738006,"user_tz":-120,"elapsed":22,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["len(decoded_imgs[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVQkk36z6Ygl","executionInfo":{"status":"aborted","timestamp":1650910738006,"user_tz":-120,"elapsed":21,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# creamos una lista de vectores (puntos en nuevo espacio latente)\n","#vectors = np.array([[-7, 5], [-5, 10], [0,0], [10, 0], [10, -5], [3, 5], [1, -5]])\n","vectors = (np.random.rand(25, 2) * 40) - 20\n","\n","# usamos el decodificador para obtener la imágenes reconstruidas a partir de los vectores\n","decoded_imgs = AE.decoder.predict(vectors)\n","print(decoded_imgs.shape)\n","decoded_imgs = [decoded_imgs[i:i+5] for i in range(0, 25, 5)]\n","draw2compare(*decoded_imgs)\n","\n","# mostramos el espacio latente resaltando los puntos que hemos muestreado\n","show_latent_space(AE, x_test_mnist, vectors)\n"]},{"cell_type":"markdown","metadata":{"id":"LTWsX-YM6Ygm"},"source":["De manera similar podemos crear una visualización para tener una mejor idea de como se distribuye nuestro espacio latente. Solo tenemos que muestrear a lo largo del espacio y crear un grid de imágenes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuccDXrr6Ygm","executionInfo":{"status":"aborted","timestamp":1650910738007,"user_tz":-120,"elapsed":22,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["sample_latent_space(AE, (15, -10), (-10, 10), n=30)"]},{"cell_type":"markdown","metadata":{"id":"xsmxzQ716Ygm"},"source":["Como vemos hay gran cantidad de imagenes que no parecen dígítos. Esto es debido a que hemos hecho una barrido lineal, lo que nos habrá llevado a muchas zonas vacías del espacio latente que no tienen ninguna representación cercana en el conjunto de entrenamiento. "]},{"cell_type":"markdown","metadata":{"id":"CKP_v94b6Ygm"},"source":["El segundo problema (digitos con una pésima reconstrucción) lo  podemos mitigar aumentand la dimensión del espacio latente a costa de perder poder de representación:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MAFPMYXh6Ygn","executionInfo":{"status":"aborted","timestamp":1650910738008,"user_tz":-120,"elapsed":23,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["AE5 = Autoencoder(\n","    input_dim = (28,28,1)\n","    , encoder_conv_filters = [32,64,64,64]\n","    , encoder_conv_kernel_size = [3,3,3,3]\n","    , encoder_conv_strides = [1,2,2,1]\n","    , decoder_conv_filters = [64,64,32,1]\n","    , decoder_conv_kernel_size = [3,3,3,3]\n","    , decoder_conv_strides = [1,2,2,1]\n","    , activation = 'relu'\n","    , z_dim = 5)\n","\n","AE5.model.compile(optimizer=Adam(), loss = 'mean_squared_error', metrics=[ssim_loss])\n","\n","AE5.model.fit(x=x_train_mnist, y=x_train_mnist, \n","             validation_data=(x_test_mnist, x_test_mnist),\n","             batch_size=128, shuffle=True, epochs=50,\n","             callbacks=[PlotLossesKerasTF()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vz_2_ClN6Ygn","executionInfo":{"status":"aborted","timestamp":1650910738009,"user_tz":-120,"elapsed":22,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["n = 25\n","decoded_imgs = AE5.model.predict(x_test_mnist[:n])\n","imgs = [decoded_imgs[i:i+5] for i in range(0, 25, 5)]\n","draw2compare(*imgs, figsize=(10, 10))"]},{"cell_type":"markdown","metadata":{"id":"cIk8L4SQ6Ygn"},"source":["# Algunas aplicaciones\n","\n","### Denoising autoencoders \n","\n","Los *denoising autoencoders* o autoencoders para eliminación/reducción de ruido son una aplicación particular de los autoencoders. Desde el punto de vista de la estructura de la red no tienen nada de particular. Lo única particularidad es el modo de entrenamiento, en concreto, los datos de entrada. Así que nos vamos a crear nuestra red como en el ejemplo anterior:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWhj8xwo6Ygo","executionInfo":{"status":"aborted","timestamp":1650910738009,"user_tz":-120,"elapsed":22,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["DAE_MNIST = Autoencoder(\n","    input_dim = (28,28,1)\n","    , encoder_conv_filters = [32,64,64,64]\n","    , encoder_conv_kernel_size = [3,3,3,3]\n","    , encoder_conv_strides = [1,2,2,1]\n","    , decoder_conv_filters = [64,64,32,1]\n","    , decoder_conv_kernel_size = [3,3,3,3]\n","    , decoder_conv_strides = [1,2,2,1]\n","    , activation = 'relu'\n","    , z_dim = 16)\n","\n","DAE_MNIST.model.compile(optimizer=Adam(), loss = 'mean_squared_error', metrics=[ssim_loss])"]},{"cell_type":"markdown","metadata":{"id":"-M2IXwkc6Ygo"},"source":["Vamos a seguir jugando con datos de los que no tenemos etiquetas asociadas a las muestras (o al menos no las vamos a usar en la fase de entrenamiento). Lo que cambia en esta aplicación es que vamos a aplicar una distorción a la imagen de entrada, pero la imagen que esperamos que reconstruya será la imagen limpia:\n","\n","<img src='https://hdvirtual.us.es/discovirt/index.php/s/jYsMLB5WTRS9Rg7/preview' width='70%'>\n","\n","Vamos a seguir con el conjunto de datos MNIST. En primer lugar vamos a definir una función para agregar ruido a un conjunto de datos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTHiOgmt6Ygo","executionInfo":{"status":"aborted","timestamp":1650910738010,"user_tz":-120,"elapsed":22,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["def add_noise(data, center=0.5, std=0.5, hard=False):\n","    # ruido usando una distribución normal centrada en 0.5 y con std 0.5\n","    noise = np.random.normal(loc=0.5, scale=std, size=data.shape)\n","    \n","    if hard:\n","        return np.clip(data + noise, 0., 1.)\n","    else:\n","        img2 = data*2\n","        n4 = np.clip(np.where(img2 <= 1, (img2*(1 + noise*0.4)), (1-img2+1)*(1 + noise*0.4)*-1 + 2)/2, 0,1)\n","        return n4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gb6KU0o06Ygp","executionInfo":{"status":"aborted","timestamp":1650910738011,"user_tz":-120,"elapsed":23,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["draw2compare(add_noise(x_test_mnist[:10], hard=True), x_test_mnist[:10])"]},{"cell_type":"markdown","metadata":{"id":"eP175ywh6Ygp"},"source":["Es una función muy sencilla que agrega un ruido aleatorio usando una distribución normal centrada en 0.5 y con desviación estanda 0.5. Se puede apreciar que las imágenes son bastante ruidosas. \n","\n","Por tanto, a la hora de entrenar le daremos a la red las imágenes de le primera fila y para computar el error cometido le daremos las imágenes de segunda fila."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BXOfHDhf6Ygp","executionInfo":{"status":"aborted","timestamp":1650910738012,"user_tz":-120,"elapsed":23,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["x_train_noisy = add_noise(x_train_mnist, hard=True)\n","x_test_noisy = add_noise(x_test_mnist, hard=True)\n","\n","DAE_MNIST.model.fit(x=x_train_noisy, y=x_train_mnist, \n","             validation_data=(x_test_noisy, x_test_mnist),\n","             batch_size=128, shuffle=True, epochs=50,\n","             callbacks=[PlotLossesKerasTF()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kriU2QvJ6Ygp","executionInfo":{"status":"aborted","timestamp":1650910738013,"user_tz":-120,"elapsed":24,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["DAE_MNIST.model.save_weights('dae_mnist_weights.h5')"]},{"cell_type":"markdown","metadata":{"id":"gfN3qJyp6Ygq"},"source":["Una vez finalizado el entrenamiento podemos ver la calidad de nuestro reductor de ruido:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6ZumqsZj6Ygq","executionInfo":{"status":"aborted","timestamp":1650910738013,"user_tz":-120,"elapsed":23,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["noise_samples = add_noise(x_test_mnist[:10], hard=True)\n","denoise_samples = DAE_MNIST.model.predict(noise_samples)\n","\n","draw2compare(x_test_mnist[:10], noise_samples, denoise_samples)"]},{"cell_type":"markdown","metadata":{"id":"aFnNCTjN6Ygq"},"source":["Parece muy efectivo con este conjunto de datos. ¿Y si probamos con un conjunto de datos \"más real\"?\n","\n","### Denoising autoencoder con CIFAR10\n","\n","El conjunto de datoa CIFAR10 esta compuesto por imágenes RGB de tamaño 32x32. Siguen siendo pequeñas, pero al menos vamos a probar con imágenes más complejas y a color.\n","\n","Al igual que con MNIST solo tenemos que leer el conjunto de datos con Keras y escalar las imágenes a valores entre 0 y 1:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOzuJDhj6Ygq","executionInfo":{"status":"aborted","timestamp":1650910738014,"user_tz":-120,"elapsed":24,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","\n","image_size = x_train.shape[1]\n","x_train = x_train.astype('float32') / 255\n","x_test = x_test.astype('float32') / 255"]},{"cell_type":"markdown","metadata":{"id":"8v3Smibc6Ygr"},"source":["En este caso vamos a probar con un ruido un poco más sutíl. Aplicaremos un ruido gausiano centrado en 0 y con desviación 0.4:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOy6tPSQ6Ygr","executionInfo":{"status":"aborted","timestamp":1650910738014,"user_tz":-120,"elapsed":24,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["draw2compare(x_train[:10], add_noise(x_train[:10], center=0, std=0.4))"]},{"cell_type":"markdown","metadata":{"id":"YXUF_gKX6Ygr"},"source":["Algunos cambios con respecto al ejemplo con MNIST:\n","   1. Ahora el decoficador debe generar una imagen con 3 canales (RGB) por lo que la última capa convolucional debe generar 3 mapas (filters = 3). Anteriormente, como eran imágenes en escala de grises, este valor era 1.\n","   2. El tamaño del vector latente ahora es mucho más grande. Queremos eliminar el ruido, pero no queremos perder excesiva información de la imagen de entrada.\n","   3. `ReduceLROnPlateau` implementa una reducción progresiva del learning rate cuando no se produce mejora."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3iQ4Cw6k6Ygr","executionInfo":{"status":"aborted","timestamp":1650910738015,"user_tz":-120,"elapsed":24,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["DAE_CIFAR10 = Autoencoder(\n","        input_dim = (32,32,3)\n","        , encoder_conv_filters = [32,64,64,64]\n","        , encoder_conv_kernel_size = [3,3,3,3]\n","        , encoder_conv_strides = [1,2,2,1]\n","        , decoder_conv_filters = [64,64,32,3]\n","        , decoder_conv_kernel_size = [3,3,3,3]         #(1)\n","        , decoder_conv_strides = [1,2,2,1]\n","        , activation = 'relu'\n","        , z_dim = 1024)                                 #(2)\n","\n","lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, verbose=1, min_lr=0.5e-6)   #(3)\n","\n","DAE_CIFAR10.model.compile(optimizer='adam', loss = 'mean_squared_error', metrics=[ssim_loss])\n","\n","x_train_noisy = add_noise(x_train, center=0, std=0.4)\n","x_test_noisy = add_noise(x_test,center=0, std=0.4)\n","\n","DAE_CIFAR10.model.fit(x=x_train_noisy, y=x_train, \n","             validation_data=(x_test_noisy, x_test),\n","             batch_size=128, shuffle=True, epochs=50,\n","             callbacks=[PlotLossesKerasTF()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3YN2PrM76Ygs","executionInfo":{"status":"aborted","timestamp":1650910738016,"user_tz":-120,"elapsed":25,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["DAE_CIFAR10.model.save_weights('dae_cifar10_weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"14kE8_O66Ygs","executionInfo":{"status":"aborted","timestamp":1650910738016,"user_tz":-120,"elapsed":25,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["noise_samples = add_noise(x_test[:10], center=0, std=0.4)\n","denoise_samples = DAE_CIFAR10.model.predict(noise_samples)\n","\n","draw2compare(x_test[:10], noise_samples, denoise_samples)"]},{"cell_type":"markdown","metadata":{"id":"2ThBAu-86Ygs"},"source":["## Autoencoder para colorear\n","\n","En esta aplicación vamos a enseñar al autoencoder a colorear imágenes en blanco y negros. El proceso es muy similar al caso anterior. Tomaremos las imágenes de nuestro conjunto de datos y a la entrada le vamos a aplicar una transformación, en este caso la convertiremos a blanco y negro. La salida esperada, que le daremos a la red, será la imagen original con en el caso anterior:\n","\n","<img src='https://hdvirtual.us.es/discovirt/index.php/s/pkkKpCm25XbjCKH/preview' width='70%' />\n","\n","Vamos a usar una función que pasará nuestras imágenes en color de CIFAR10 a blanco y negro:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"Qi1KxGqX6Ygs","executionInfo":{"status":"aborted","timestamp":1650910738017,"user_tz":-120,"elapsed":25,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["#from utils import rgb2gray\n","\n","\n","def rgb2gray(data):\n","    # grayscale = 0.299*red + 0.587*green + 0.114*blue\n","    factors = [0.299, 0.587, 0.114]\n","    return np.dot(data[...,:3], factors).reshape(list(data.shape[:-1]) + [1])\n","        \n","    \n","draw2compare(rgb2gray(x_train[:10]), x_train[:10])"]},{"cell_type":"markdown","metadata":{"id":"lFICD4z66Ygt"},"source":["La primera fila serán las imágenes de entrada a nuestro autoencoder y la segunda fila la reconstrucción esperada.\n","\n","El proceso de entrenamiento es el mismo que en el caso anterior. Únicamente se ha cambiado el punto (3) donde se transforman los datos que vamos a usar de entrada. Sobre la construcción del autoencoder en (1) hemos cambiado la dimensión de entrada. Ahora solo tenemos un canal, ya que la entrada es en escala de grises. En (2) seguimos teniendo 3 canales de salida ya que esperamos la imagen en RGB."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZO2F-Kq6Ygt","executionInfo":{"status":"aborted","timestamp":1650910738018,"user_tz":-120,"elapsed":26,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["CAE_CIFAR10 = Autoencoder(\n","        input_dim = (32,32,1)                          #(1)\n","        , encoder_conv_filters = [32,64,64,64]\n","        , encoder_conv_kernel_size = [3,3,3,3]\n","        , encoder_conv_strides = [1,2,2,1]\n","        , decoder_conv_filters = [64,64,32,3]\n","        , decoder_conv_kernel_size = [3,3,3,3]         #(2)\n","        , decoder_conv_strides = [1,2,2,1]\n","        , activation = 'relu'\n","        , z_dim = 1024)                                \n","\n","CAE_CIFAR10.model.compile(optimizer='adam', loss = 'mean_squared_error', metrics=[ssim_loss])\n","\n","x_train_gray = rgb2gray(x_train)                     #(3)\n","x_test_gray = rgb2gray(x_test)\n","\n","CAE_CIFAR10.model.fit(x=x_train_gray, y=x_train, \n","             validation_data=(x_test_gray, x_test),\n","             batch_size=128, shuffle=True, epochs=20,\n","             callbacks=[PlotLossesKerasTF()])"]},{"cell_type":"markdown","metadata":{"id":"vT-nOISt6Ygu"},"source":["# Referencias\n","\n"," - Generative Deep Learning. Teaching Machines to Paint, Write, Compose and Play. David Foster. O'Reilly\n"," - Advanced Deep Learning with Keras. Rowel Atienza. Packt."]}],"metadata":{"copyright":{"author":"David Solís Martín"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"1. Auto-codificadores.ipynb","provenance":[],"toc_visible":true},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}