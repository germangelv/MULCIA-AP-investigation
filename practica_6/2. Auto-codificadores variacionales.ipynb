{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"HNaC-Xbz9wWH","executionInfo":{"status":"ok","timestamp":1650911083809,"user_tz":-120,"elapsed":6210,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"61b4e44a-e386-44c1-b222-edfc045b0e52"},"outputs":[{"output_type":"stream","name":"stdout","text":["Estoy en Google CoLab\n","Collecting livelossplot\n","  Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n","Requirement already satisfied: numpy<1.22 in /usr/local/lib/python3.7/dist-packages (from livelossplot) (1.21.6)\n","Collecting ipython==7.*\n","  Downloading ipython-7.32.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 5.8 MB/s \n","\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (5.1.1)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n","\u001b[K     |████████████████████████████████| 381 kB 43.9 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (57.4.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.2.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.7.5)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.6.1)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.1.3)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.18.1)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.4.2)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython==7.*->livelossplot) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython==7.*->livelossplot) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.5)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (4.2.0)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.8)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.4.2)\n","Installing collected packages: prompt-toolkit, ipython, livelossplot\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.32.0 which is incompatible.\u001b[0m\n","Successfully installed ipython-7.32.0 livelossplot-0.5.5 prompt-toolkit-3.0.29\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","prompt_toolkit"]}}},"metadata":{}}],"source":["import os\n","try:\n","    from google.colab import drive\n","    COLAB = True\n","    print(\"Estoy en Google CoLab\")\n","    %tensorflow_version 2.x\n","    !pip install livelossplot\n","except:\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n","    print(\"No estoy en Google CoLab\")\n","    COLAB = False"]},{"cell_type":"markdown","metadata":{"id":"2tIdemHF9wWP"},"source":["# Autoencodificadores variacionales"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"aBtjLPyv9wWS","executionInfo":{"status":"error","timestamp":1650911088182,"user_tz":-120,"elapsed":4386,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"87dee099-9a23-42c2-8624-0747c66e5459"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b9156a37ecf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbinary_crossentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mlivelossplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPlotLossesKerasTF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Adam' from 'keras.optimizers' (/usr/local/lib/python3.7/dist-packages/keras/optimizers.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["# Cargamos las librerías necesarias para la práctica\n","from keras import models\n","from keras import backend as K\n","from keras.models import Model\n","from keras.layers import Input, Conv2D, Dense, Flatten, LeakyReLU, Activation, Dropout\n","from keras.layers import Layer, Reshape, Conv2DTranspose, Lambda, ReLU, BatchNormalization\n","from keras.metrics import binary_crossentropy\n","from keras.optimizers import Adam\n","try:\n","  from livelossplot import PlotLossesKerasTF\n","except:\n","  !pip install livelossplot\n","  from livelossplot import PlotLossesKerasTF\n","    \n","try:\n","  import tqdm\n","except:\n","  !pip install tqdm\n","  import tqdm\n","    \n","from keras.utils import plot_model\n","from keras.datasets import mnist, cifar10\n","from tqdm import tqdm\n","from io import BytesIO, StringIO\n","import numpy as np\n","import pathlib\n","import cv2\n","import matplotlib\n","import os\n","import random\n","import re\n","import tensorflow as tf\n","# %matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","matplotlib.rcParams['figure.figsize'] = (15,5) # Para el tamaño de la imagen\n","matplotlib.rcParams['figure.figsize'] = (15,5)\n","\n","import seaborn as sns\n","import pandas as pd\n","from utils import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8FgoRSW9wWV","executionInfo":{"status":"aborted","timestamp":1650911088163,"user_tz":-120,"elapsed":21,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# MNIST dataset\n","(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n","\n","image_size = x_train_mnist.shape[1]\n","x_train_mnist = np.reshape(x_train_mnist, [-1, image_size, image_size, 1])\n","x_test_mnist = np.reshape(x_test_mnist, [-1, image_size, image_size, 1])\n","x_train_mnist = x_train_mnist.astype('float32') / 255\n","x_test_mnist = x_test_mnist.astype('float32') / 255"]},{"cell_type":"markdown","metadata":{"id":"HUaatjfj9wWX"},"source":["## Autoencoders variacionales\n","\n","El espacio latente generado por los autoencoders sufren de falta de simetría, las muestras no se distribuyen de manera equitativa en el espacio y hay mucho espacios vacios. Esto provoca que el generador genere en gran parte del espacio imágenes irreconocibles. Los autoencoders variacionales intentan solventar estas deficiencias. \n","\n","<img src='https://hdvirtual.us.es/discovirt/index.php/s/NE9rr6KH9CTMSqN/preview' width='70%' />\n","\n","Para ello el vector latente se mapeará de una distribución normal multivariante alrededor de un punto en el espacio latente:\n","\n","$$z = \\mu + \\sigma\\epsilon$$\n","\n","$z$ es nuestro vector latente, que se genera a partir de $\\mu$ y $\\sigma$ que deberá aprender el autoencoder. $\\epsilon$ se muestrea de una distribució nnormal estándar. \n","\n","Como vemos en el gráfico anterior, tendremos que modificar la codificador, pero el decodificador sigue manteniendo la misma estructura, así que vamos a partir de nuestra clase `Autoencoder` para implementa una nueva clase `VariatioalAutoencoder`"]},{"cell_type":"markdown","metadata":{"id":"2XI6-uR_9wWZ"},"source":["Otro elemento que cambia en los autoencoders variacionales es funcion de coste. Ahora sera:\n","\n","$$VAE_{loss} = \\eta_1 METRICA + \\eta_2 D_{KL}$$\n","\n","donde:\n","\n","$$D_{KL} = \\frac{1}{2} \\sum^k_i (1 + log(\\sigma_i^2) - \\mu_i^2 - \\sigma_i^2)$$\n","\n","y *METRICA* puede ser *MSE*, *Binary corss entropy*, *SSIM*, .... "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"da78Jvy49wWb","executionInfo":{"status":"aborted","timestamp":1650911088164,"user_tz":-120,"elapsed":22,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["class VariationalAutoencoder(Autoencoder):\n","    \n","    def __init__(self, eta=1, alpha=0.001, loss_type='cross_entropy', **kwargs):\n","        super(VariationalAutoencoder, self).__init__(**kwargs)\n","        \n","        self.eta = eta\n","        self.alpha = alpha\n","        self.loss_type = loss_type\n","        \n","        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n","        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n","            name=\"reconstruction_loss\"\n","        )\n","        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n","        \n","    def _create_latent_vector(self, encoder_input, x):\n","        \n","        self._mu = Dense(self.z_dim, name='mu')(x)                  #(1)                        \n","        self._log_var = Dense(self.z_dim, name='log_var')(x)\n","        \n","        self._encoder_mu_log_var = Model(encoder_input, (self._mu, self._log_var))    #(2)\n","        \n","        def sampling(args):\n","            mu, log_var = args\n","            epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n","            return mu + K.exp(log_var / 2) * epsilon\n","        \n","        self.encoder_output = Lambda(sampling, name='encoder_output')([self._mu, self._log_var])   #(3)\n","\n","        return self.encoder_output\n","    \n","    def call(self, data):\n","        (z_mean, z_log_var), z = self._encoder_mu_log_var(data), self.encoder(data)\n","        reconstruction = self.decoder(z)\n","        data = data\n","        \n","        if self.loss_type == 'cross_entropy':\n","            reconstruction_loss = tf.reduce_mean(\n","                tf.reduce_sum(\n","                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n","                )\n","            )\n","        elif self.loss_type == 'ssim':\n","            reconstruction_loss = 1 - tf.reduce_mean(tf.image.ssim(data, reconstruction, 2.0))\n","\n","        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n","        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","        total_loss = self.eta * reconstruction_loss +  self.alpha * kl_loss\n","        \n","        # actualizamos las métricas\n","        self.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n","        self.add_metric(total_loss, name='loss', aggregation='mean')\n","        self.add_metric(reconstruction_loss, name='reconstruction_loss', aggregation='mean')\n","        return reconstruction\n","    \n","    def train_step(self, data):\n","\n","        with tf.GradientTape() as tape:\n","            (z_mean, z_log_var), z = self._encoder_mu_log_var(data), self.encoder(data)\n","            reconstruction = self.decoder(z)\n","            data = data[0]\n","\n","            if self.loss_type == 'cross_entropy':\n","                reconstruction_loss = tf.reduce_mean(\n","                    tf.reduce_sum(\n","                        keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n","                    )\n","                )\n","            elif self.loss_type == 'ssim':\n","                reconstruction_loss = 1 - tf.reduce_mean(tf.image.ssim(data, reconstruction, 2.0))\n","\n","            \n","            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n","            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","            total_loss = self.eta * reconstruction_loss +  self.alpha * kl_loss\n","            \n","        # obtenemos los gradientes        \n","        grads = tape.gradient(total_loss, self.trainable_weights)\n","        \n","        # actualizamos los pesos\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","\n","        # actualizamos las métricas\n","        self.total_loss_tracker.update_state(total_loss)\n","        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n","        self.kl_loss_tracker.update_state(kl_loss)\n","        \n","        return {\n","            \"loss\": self.total_loss_tracker.result(),\n","            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n","            \"kl_loss\": self.kl_loss_tracker.result(),\n","        }"]},{"cell_type":"markdown","metadata":{"id":"-kqpmyox9wWd"},"source":["   1. En vez de conectar directamente el vector aplanado al espacio latente se conecta a dos capas densas representante de $\\mu$ y $\\sigma$. Guardamos las capas en atributos de la clase porqué nos hará falta para computar la función de coste.\n","   2. Este modelo Keras auxiliar nos sirve para obtener $\\mu$ y $\\sigma$ a partir de una imagen de entrada.\n","   3. La capa lambda usa la función `sampling` para muestrear de la distribución definida por $\\mu$ y $\\sigma$.\n","   \n","Para comparar la diferencia, nos vamos a crear un autoencoder variacional con la misma estructura base que en el ejemplo del autoencoder para MNIST:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q6uIQOnl9wWe","executionInfo":{"status":"aborted","timestamp":1650911088165,"user_tz":-120,"elapsed":23,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["VAE_MNIST = VariationalAutoencoder(\n","        eta = 1,\n","        alpha = 1/50,\n","        loss_type='ssim',\n","         input_dim = (28,28,1)\n","        , encoder_conv_filters = [32,64,64,64]\n","        , encoder_conv_kernel_size = [3,3,3,3]\n","        , encoder_conv_strides = [1,2,2,1]\n","        , decoder_conv_filters = [64,64,32,1]\n","        , decoder_conv_kernel_size = [3,3,3,3]\n","        , decoder_conv_strides = [1,2,2,1]\n","        , z_dim = 2\n","        , activation = 'relu')       "]},{"cell_type":"markdown","metadata":{"id":"D5t9YgLq9wWf"},"source":["Vamos a ver que se corresponde con la estructura esperada usando el método `plot_model` de Keras:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4pOpf_z9wWg","executionInfo":{"status":"aborted","timestamp":1650911088165,"user_tz":-120,"elapsed":23,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["plot_model(VAE_MNIST.model, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"dzdDQikt9wWh"},"source":["Vamos a entrenar nuestra VAE para comparar el espacio latente generado:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aymoMKue9wWi","executionInfo":{"status":"aborted","timestamp":1650911088166,"user_tz":-120,"elapsed":24,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["VAE_MNIST.compile(optimizer=Adam(0.0005))\n","\n","VAE_MNIST.fit(x=x_train_mnist, \n","             validation_data=(x_test_mnist, x_test_mnist),\n","             batch_size=32, shuffle=True, epochs=25,\n","             callbacks=[PlotLossesKerasTF()])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFPit6fb9wWi","executionInfo":{"status":"aborted","timestamp":1650911088167,"user_tz":-120,"elapsed":24,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["VAE_MNIST.model.save_weights('vae_mnist_weights.h5')"]},{"cell_type":"markdown","metadata":{"id":"9G7BsWqf9wWj"},"source":["Vamos a mostrar ahora nuestro espacio latente:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UBLUidX9wWj","executionInfo":{"status":"aborted","timestamp":1650911088167,"user_tz":-120,"elapsed":24,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["import utils as u\n","u.show_latent_space(VAE_MNIST, x_test_mnist, y_test_mnist)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"31qjp0B89wWk","executionInfo":{"status":"aborted","timestamp":1650911088168,"user_tz":-120,"elapsed":25,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["u.show_latent_space(VAE_MNIST, x_test_mnist, y_test_mnist)\n"]},{"cell_type":"markdown","metadata":{"id":"VO-nVuCY9wWl"},"source":["Si nos centramos más en $0,0$ y en una una desviación de aproximadamente 1 veremos como hemos mejorando nuestro espacios latente:"]},{"cell_type":"markdown","metadata":{"id":"FOwhZFAc9wWl"},"source":["Como en el caso anterior podemos hacer un muestreo de nuestro espacio latente. Dado que hemos hemos *obligado* a nuestro autoencoder a que el espacio esté centrado en 0 y tengan una deviación de 1 aproximadamente, podemos muestrear unicamente en el rango -1, 1:"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":false,"id":"mqGVZ4gT9wWl","executionInfo":{"status":"aborted","timestamp":1650911088169,"user_tz":-120,"elapsed":26,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["sample_latent_space(VAE_MNIST, x_range=(-1.3,1.3), y_range=(-1.3,1.3))"]},{"cell_type":"markdown","metadata":{"id":"NmjcFGlf9wWm"},"source":["## Usando autoencoders variacionales para generar caras\n","\n","En este ejercicio usaremos el conjunto de datos [CelebFaces Attributes (CelebA)](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) para entrenar nuestro siguiente autoencoder. Este conjunto de datos compuesto por aproximadamente 200.000 imágenes de caras de famosos, cada una de ellas anotadas con una o más etiquetas del tipo: sonriendo, con gafas, llevando sombrero, etc...\n","\n","<img src='https://hdvirtual.us.es/discovirt/index.php/s/yysHBNpofeGKdyG/download' width='70%' />\n","\n","No vamos a usar dichas etiquetas para entrenar nuestra VAE, pero nos serán de utilidad para llevar a cabo ciertas operaciones en el espacio latente.\n","\n","En primer lugar debemos descargar y descomprimir el conjunto de datos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pmkxADhs9wWn","executionInfo":{"status":"aborted","timestamp":1650911088169,"user_tz":-120,"elapsed":25,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["data_path = tf.keras.utils.get_file(\n","    \"celeba.tar.gz\",\n","    \"https://hdvirtual.us.es/discovirt/index.php/s/9KMEFpajdzgDAC9/download\",\n","    extract=True,\n","    archive_format='tar'\n",")\n","\n","DATA_FOLDER = pathlib.Path(data_path).parent / 'celeba' \n","\n","IMG_FOLDER = os.path.join(DATA_FOLDER, 'images')\n","att_file = os.path.join(DATA_FOLDER, 'att.csv')\n","\n","# obtenemos los nombres de los archivos de imágenes\n","file_names = os.listdir(IMG_FOLDER)\n","\n","print('Total de imagenes: %d' % len(file_names))"]},{"cell_type":"markdown","metadata":{"id":"OUY6ytm69wWn"},"source":["Lo siguiente será crear nuestro VAE. Trabajamos con imágenes en color por lo que la entrada y la salida tendrán 3 canales. En este caso vamos a normalizar la salida de cada capa convolucional y vamos a aplicar un dropout del 25%.\n","\n","El resto es prácticamente igual a los ejemplos anteriores:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzsWEoIW9wWo","executionInfo":{"status":"aborted","timestamp":1650911088170,"user_tz":-120,"elapsed":26,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["INPUT_DIM = (128, 128, 3)\n","\n","VAE_CELEBA = VariationalAutoencoder(\n","                        eta = 1,\n","                        alpha = 1e-3,\n","                        loss_type='ssim',\n","                        input_dim = INPUT_DIM\n","                        , encoder_conv_filters=[32,64,64, 64]\n","                        , encoder_conv_kernel_size=[3,3,3,3]\n","                        , encoder_conv_strides=[2,2,2,2]\n","                        , decoder_conv_filters=[64,64,32,3]\n","                        , decoder_conv_kernel_size=[3,3,3,3]\n","                        , decoder_conv_strides=[2,2,2,2]\n","                        , activation = ReLU\n","                        , z_dim=200\n","                        , use_batch_normalization=True\n","                        , dropout=0.25)\n","\n","VAE_CELEBA.compile(optimizer=Adam(0.0005))"]},{"cell_type":"markdown","metadata":{"id":"1P3ZX5EE9wWp"},"source":["Este conjunto de datos tiene un tamaño total de 1.3GB en disco. El problema viene cuando convertimos la imágenes a array numpy de floats. El tamaño se multiplica llegando a pesar más de 30GB o más. Dado que no podemos cargar en memoria todas las imágenes, Keras nos da herramientas para trabajar con alimentación dinámica. Esto es, vamos a ir leyendo de disco los batches conforme vayan haciendo falta.\n","\n","El procedimiento es muy sencillo. Usamos la clase `ImageDataGenerator` indicando el escalado y el porcentaje del conjunto de datos que queremos reservar para validación. Con el método `flow_from_directory` indicamos la carpeta donde se encuentran las imágenes, el tamaño del batch y con el atributo `subset` establecemos si ese flujo será de la parte reservada para entrenamietno o para validación. Por último tenemos que entrenar, que ahora lo haremos con el método `fit_generator`.\n","\n","```\n","from keras_preprocessing.image import ImageDataGenerator\n","\n","INPUT_DIM = (128,128,3)\n","BATCH_SIZE = 128\n","\n","\n","# creamos nuestro iterador indicando que tras leer la imagen se debe escalar dividiendo por 255\n","data_gen = ImageDataGenerator(rescale=1./255, validation_split=0.3)\n","\n","train_data_flow = data_gen.flow_from_directory(\n","    DATA_FOLDER, \n","    target_size = INPUT_DIM[:2], \n","    batch_size = BATCH_SIZE, \n","    shuffle = True, \n","    class_mode = 'input', \n","    subset = \"training\")\n","\n","val_data_flow = data_gen.flow_from_directory(\n","    DATA_FOLDER, \n","    target_size = INPUT_DIM[:2], \n","    batch_size = BATCH_SIZE, \n","    shuffle = True, \n","    class_mode = 'input', \n","    subset = \"validation\")\n","\n","steps_per_epoch = int((len(file_names) * 0.7) / BATCH_SIZE)\n","\n","VAE_CELEBA.model.fit_generator(train_data_flow, validation_data=val_data_flow, shuffle = True, \n","                               epochs = 200, callbacks = [PlotLossesKeras()], \n","                               steps_per_epoch=steps_per_epoch \n","    )\n","```\n","\n","Con este código cada época tardaría unos 3min. Si vamos a ejecutar 200 epocas, no llevaría el entrenamiento 600 min sin contar la evaluación. \n","\n","Recordamos que el conjunto de datos en disco ocupa 1.3GB. ¿Y si lo precargamos en memoria en binario? De esa forma evitamos la lectura a disco de los mismo archivos en cada época y dado que son solo 1.3GB no deberíamos tener muchos problemas de memoria. \n","\n","Este procedimiento los podemos hacer con una sola línea de código:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Voxn7BK9wWp","executionInfo":{"status":"aborted","timestamp":1650911088170,"user_tz":-120,"elapsed":26,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["X = [open(os.path.join(DATA_FOLDER, 'images', f), 'rb').read() for f in tqdm(file_names)]"]},{"cell_type":"markdown","metadata":{"id":"rv6fplv19wWq"},"source":["Una vez leidos los archivos en memoria tendriamos que ir convirtiendolos a arrays de numpy. Vamos a ver cuando tardaríamos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ow_RAgru9wWq","executionInfo":{"status":"aborted","timestamp":1650911088171,"user_tz":-120,"elapsed":27,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["import imageio\n","for im in tqdm(X):\n","    b = BytesIO(im)\n","    file_bytes = np.asarray(bytearray(BytesIO(im).read()), dtype=np.uint8)\n","    im = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n","    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JUkRtgHZ9wWr"},"source":["Se tardaría 1 min y 21 seg aproximadamente. Por lo que estariamos reduciendo un 50% el tiempo de procesamiento.\n","\n","Para aprovechar este truco tenemos que crearnos una clase que haga de iterador, como la clase `ImageDataGenerator` de Keras.\n","\n","La implementación de clase sería la siguiente:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HF6QRdlr9wWt","executionInfo":{"status":"aborted","timestamp":1650911088173,"user_tz":-120,"elapsed":29,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["class CelebaInMemoryIterator(tf.keras.utils.Sequence):\n","       \n","    def __init__(self, im_bytes, batch_size, shuffle=True):\n","        self.im_bytes = im_bytes\n","        self.batch_size = batch_size\n","        self.shuffle = shuffle\n","\n","        self.__len = int(len(self.im_bytes) / self.batch_size)\n","        \n","        self.reset()\n","\n","    def __len__(self):\n","        return self.__len\n","\n","    def __getitem__(self, index):\n","        # comienzo y final del batch\n","        i = index * self.batch_size\n","        j = i + self.batch_size\n","        \n","        # indixes del batch\n","        indexes = self.__indexes[i:j]\n","        \n","        # imagenes binarias del batch\n","        batch = [self.im_bytes[k] for k in indexes]\n","\n","        # lista final de arrays\n","        ims = []\n","   \n","        # convertimos a numpy arrays\n","        for im in batch:\n","            b = BytesIO(im)\n","            file_bytes = np.asarray(bytearray(BytesIO(im).read()), dtype=np.uint8)\n","            im = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)\n","            im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","            im = cv2.resize(im, (128, 128)) \n","            ims.append(im)\n","\n","        # escalamos a valores entre 0 y 1\n","        ims = np.array(ims).astype('float') / 255\n","        \n","        \n","        return ims, ims\n","\n","\n","    def on_eponch_end(self):\n","        self.reset()\n","        \n","    def reset(self):\n","        self.__indexes = list(range(len(self.im_bytes)))           \n","        if self.shuffle:\n","            self.__indexes = random.sample(self.__indexes, len(self.__indexes))\n","        self.__current_index = 0\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self, *args, **kwargs):\n","        # si hemos llegado al final reiniciamos\n","        if self.__current_index >= self.__len:\n","            self.reset()\n","            \n","        # cogemos el siguiente batch\n","        r =  self.__getitem__(self.__current_index)\n","        \n","        # incrementamos el indice\n","        self.__current_index += 1\n","        \n","        \n","        return r\n","    "]},{"cell_type":"markdown","metadata":{"id":"lKLLa6Yz9wWu"},"source":["El método más importante es el método `__getitem__`. Este método recibe un parámetro `index` que correcto con el número de batch de nuestro conjunto de datos. Con ese número se debe preparar el batch y devolverlo.\n","\n","El método `reset` simplemente reinicia a 0 el indice y baraja los indices si el atributo `shuffle` se estableció a true. Por último el métod `next` devuelve el siguiente batch a ser procesado. Básicamente controla si es necesario reiniciar el índice y lo incrementa en cada llamada.\n","\n","Ahora podemos usar nuestra clase con `fit_generator`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iv1SyT5e9wWu","executionInfo":{"status":"aborted","timestamp":1650911088174,"user_tz":-120,"elapsed":29,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["INPUT_DIM = (128,128,3)\n","BATCH_SIZE = 128\n","\n","# dividimos el conjunto de datos entrenamiento y test\n","indexes = list(range(len(X)))\n","random.shuffle(indexes)\n","cut_point = int(len(X) * 0.7)\n","X_train = [X[i] for i in indexes[:cut_point]]\n","X_test = [X[i] for i in indexes[cut_point:]]    \n","\n","# creamos los iteradores\n","train_generator = CelebaInMemoryIterator(X_train, 32)\n","test_generator = CelebaInMemoryIterator(X_test, 128)\n","\n","# entrenamos usando los iteradores\n","VAE_CELEBA.fit(train_generator, validation_data=test_generator, \n","                               epochs = 10, callbacks = [PlotLossesKerasTF()])\n","        \n"]},{"cell_type":"markdown","metadata":{"id":"nMel33xW9wWu"},"source":["Vamos a ver la calidad de la reconstrucción."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DUCA0p-_9wWv","executionInfo":{"status":"aborted","timestamp":1650911088175,"user_tz":-120,"elapsed":30,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["predict_generator = CelebaInMemoryIterator(X_test[:10], 10, shuffle=True)\n","x, _ = predict_generator.__next__()\n","draw2compare(x, VAE_CELEBA.model.predict(x))"]},{"cell_type":"markdown","metadata":{"id":"y2zv4eCz9wWw"},"source":["Se aprecia que la VAE ha capturado elementos clave de la cara (el ángulo de la cara, el peinado, la expresión, etc.) y que los detalles más finos se pierden. Aun así, es importante recordar que el objetivo no es realizar una reconstrucción perfecta (aunque sería deseable). El objetivo principal es poder hacer un muestreo en el espacio latente.\n","\n","Vamos a realizar un muestreo del espacio:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUsEMDhD9wWw","executionInfo":{"status":"aborted","timestamp":1650911088176,"user_tz":-120,"elapsed":31,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["rows = 3\n","cols = 10\n","y = tuple([VAE_CELEBA.decoder.predict(np.random.normal(size=(cols, VAE_CELEBA.z_dim))) for _ in range(rows)])\n","len(y), len(y[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"025BBQQk9wWx","executionInfo":{"status":"aborted","timestamp":1650911088177,"user_tz":-120,"elapsed":31,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["draw2compare(*y)"]},{"cell_type":"markdown","metadata":{"id":"qPZoKQxH9wWx"},"source":["Es sorprendente como la VAE es capaz de crear imágenes de personas a partir de puntos tomados aleatoriamente.\n","\n","### Aritmética en el espacio latente\n","\n","Un beneficio de las representaciones en el espacio latente vectorial es que podemos llevar a cabo operaciones vectorailes y después decodificar el resultado de dicha operación.\n","\n","Por ejemplo, supongamos que queremos hacer que una imagen de una alguien que tiene una expesión de tristeza la queremos convertir en sonriente. Para poder hacer esto necesitamos un vector latente que apunte en la dirección del espacio en el cual se incrementa la sonrisa. Si agregamos este vector a otro vector latente obtendremos un vector cuya imagen asociada será más sonriente que la orignal.\n","\n","¿Como encontramos esos vectores direccionales? Para ello vamos a necesitar el conjunto de datos atributos. Cada imagen tiene asociada un conjunto de atributos, en ellos uno de es si esta sonriendo o no. Si calculamos la media de los vectores latentes de imágenes sonrientes y le restamos la media de los vectores latentes de imágnes no sonrientes obtedremos nuestro vector direccional que apunta desde *no sonriente* a *sonriente*.\n","\n","El formato de este archivo un es poco especial y tenemos que hacerle unas modficaciones antes de poder leerlo. Un vez lo hemos leido vamos a reordenarlo que siga el mismo orden que nuestra lista de ficheros:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhM3omvJ9wWy","executionInfo":{"status":"aborted","timestamp":1650911088177,"user_tz":-120,"elapsed":31,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# abrimos el archivo en modo lectura\n","with open(att_file, 'r') as fp:\n","    # eliminamos la primera línea, es el número de imágene el dataset (202599)\n","    lines = fp.readlines()[1:]\n","    # la siguiente linea es el nombre de cada columna  pero hay un espacio que demos eliminar y agregar\n","    # una columna al principio para el nombre del fichero de imagen\n","    lines[0] = ('File_Name ' + lines[0]).strip() + '\\n'\n","    # por último sustituimos los espacios por comas\n","    lines = [re.sub('[ ]+', ',', l) for l in lines]\n","    \n","# leemos (parseamos) con pandas usando StringIO ya que tenemos el archivo en memoria\n","df_data = pd.read_csv(StringIO(''.join(lines)))\n","\n","# reordenamos el conjunto de datos para que tenga el mismo orden que nuestra lista de ficheros\n","df_data = df_data.set_index('File_Name').loc[file_names].reset_index()\n","\n","# mostramos las primreas filas del conjunto de datos\n","df_data.head()"]},{"cell_type":"markdown","metadata":{"id":"D0Owxzy29wWy"},"source":["Como vemos tenemos una ficha por cada fichero identificado por su nombre. Cada columna corresponde a una caracterísca de esa imagen. Si la imagen tiene ese atributo está marcado con un 1, si no lo tiene está marcado con -1.\n","\n","Lo primero que vamos a hacer es seleccionar un atributo y calcular el vector medio de todos los vectores latentes que tienen ese atributo y los que no lo tienen. Para ello primero debemos obtener todos los vectores latentes. Vamos a usar nuestro iterador para ir obteniendo los vectores latentes e ir computando la media batch a batch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iu595PIs9wWy","executionInfo":{"status":"aborted","timestamp":1650911088178,"user_tz":-120,"elapsed":31,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# vamos a probar con el atributo smiling\n","attribute = 'Smiling'\n","\n","# tomamos los índices de que aquellas imágenes marcadas como 'soriendo' y aquellas que no\n","with_att_files = df_data[df_data[attribute] == 1].index\n","without_att_files = df_data[df_data[attribute] == -1].index\n","\n","# creamos dos listas, una con los binarios de las imagnes marcadas como 'sonriendo' y la que no\n","X_with_att = [X[i] for i in with_att_files]\n","X_without_att = [X[i] for i in without_att_files]\n","\n","def get_mean_vector(generator):\n","    \n","    mean_vector = None\n","    # usamos el generador para ir recorriendo los batches\n","    for i in tqdm(range(len(generator))):\n","        # obtenemos los vectors latentes del batch\n","        pred = VAE_CELEBA.encoder.predict(generator.__next__()[0])\n","        \n","        # si el primero calculamos la media de ese batch\n","        # en otro caso calculamos la media de la media de ese batch con la anterior\n","        if mean_vector is None:\n","            mean_vector = pred.mean(axis=0)\n","        else:\n","            mean_vector += pred.mean(axis=0)\n","            mean_vector /= 2\n","            \n","    return mean_vector\n","\n","# obtenenmos el vector medio de las imágenes marcadas como 'sonriendo'\n","predict_generator = CelebaInMemoryIterator(X_with_att, 128, shuffle=False)\n","with_att_mean_vector = get_mean_vector(predict_generator)\n","\n","# obtenenmos el vector medio de las imágenes marcadas como no 'sonriendo'\n","predict_generator = CelebaInMemoryIterator(X_without_att, 128, shuffle=False)\n","without_att_mean_vector = get_mean_vector(predict_generator)"]},{"cell_type":"markdown","metadata":{"id":"S7Q8--579wWz"},"source":["Vamos a visualizar esos dos vectores del espacio latente:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_Ja83Lo9wWz","executionInfo":{"status":"aborted","timestamp":1650911088179,"user_tz":-120,"elapsed":32,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["draw2compare(VAE_CELEBA.decoder.predict(np.array([with_att_mean_vector, without_att_mean_vector])))"]},{"cell_type":"markdown","metadata":{"id":"yXMn_uKQ9wW0"},"source":["Y ahora ya podemos calcular nuestro vector directional desde *no sonriente* a *sonriente*:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcsLuZ629wW0","executionInfo":{"status":"aborted","timestamp":1650911088179,"user_tz":-120,"elapsed":32,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["directional_vector = with_att_mean_vector - without_att_mean_vector"]},{"cell_type":"markdown","metadata":{"id":"8bRoX_bE9wW0"},"source":["El siguiente paso es simplemente tomar una imagen de nuestro conjunto de datos e ir sumandole o restandole nuestro vector direccional y ver el efecto:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ti_SiDnd9wW1","executionInfo":{"status":"aborted","timestamp":1650911088180,"user_tz":-120,"elapsed":33,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# obtenemos los vectores de 10 imágenes al azar\n","sample = CelebaInMemoryIterator(X, 10, shuffle=True).__next__()[0]\n","vectors = VAE_CELEBA.encoder.predict(sample)\n","\n","# esta será la proporción del atributo que añadiremos a nuestros vectores\n","factors = [-4,-3,-2,-1,0,1,2,3,4]\n","\n","# computamos la decoficación de los vectores modificador: v + f * d\n","changes = np.array([VAE_CELEBA.decoder.predict(vectors + f * directional_vector) for f in factors])\n","\n","# mostramos\n","draw2compare(*list(changes))"]},{"cell_type":"markdown","metadata":{"id":"fCzaIF4B9wW1"},"source":["Es sumamente interesante que aunque nos estamos alejando en el espacio latente desde el punto orignal la mayor parte de imagen no cambia excepto en lo que concierne a la característica que estamos modificando. Esto pone de manifiesto el poder de los autoencoders variacionales para capturar y permitirnos manipular características de alto nivel de abstracción.\n","\n","Otra operación interesante que podemos llevar a cabo es la morphing. Esta consiste en tomar dos imágenes A y B y trazar una línea recta entre ambas en el espacio latente. Moviendomos a lo largo de esta línea podemos recrear la progresiva transformación de una cara en la otra:\n","\n","Matemáticamente:\n","\n","$$\n","z_{new} = (1 - \\alpha) z_A - \\alpha z_B\n","$$\n","\n","donde $\\alpha$ es un número entre 0 y 1 que determina en que posición entre el punto A y B nos encontramos (o que aportación del vector A y el vector B hay en el vector resultante)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NQNdJ4qJ9wW2","executionInfo":{"status":"aborted","timestamp":1650911088181,"user_tz":-120,"elapsed":33,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["# Tomamos una lista de vectores A y vectores B\n","sample = CelebaInMemoryIterator(X, 10, shuffle=True).__next__()[0]\n","vectors_A = VAE_CELEBA.encoder.predict(sample)\n","\n","sample = CelebaInMemoryIterator(X, 10, shuffle=True).__next__()[0]\n","vectors_B = VAE_CELEBA.encoder.predict(sample)\n","\n","images = [VAE_CELEBA.decoder.predict(vectors_A)]\n","for alpha in np.arange(0,1,0.1)[1:]:\n","    z_new = VAE_CELEBA.decoder.predict((1-alpha)*vectors_A + alpha*vectors_B)\n","    images.append(z_new)\n","images.append(VAE_CELEBA.decoder.predict(vectors_B))\n","\n","# mostramos\n","draw2compare(*images)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A9JGw9pj9wW2"},"source":["La transición se realiza suavemente incluiso cuando se están alterando varias características simultaneamente gracias a las continuidad del espacio latente."]},{"cell_type":"markdown","metadata":{"id":"xgBdlt_w9wW2"},"source":["# Referencias\n","\n"," - Generative Deep Learning. Teaching Machines to Paint, Write, Compose and Play. David Foster. O'Reilly\n"," - Advanced Deep Learning with Keras. Rowel Atienza. Packt."]}],"metadata":{"copyright":{"author":"David Solís Martín"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"2. Auto-codificadores variacionales.ipynb","provenance":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}