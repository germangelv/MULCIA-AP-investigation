{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JGeld2C9DtcF","executionInfo":{"status":"ok","timestamp":1652120574372,"user_tz":-120,"elapsed":10174,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"a5943e3c-b9b1-4d52-af21-b938c6d06f49","colab":{"base_uri":"https://localhost:8080/","height":1000}},"outputs":[{"output_type":"stream","name":"stdout","text":["Estoy en Google CoLab\n","Collecting livelossplot\n","  Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n","Collecting ipython==7.*\n","  Downloading ipython-7.33.0-py3-none-any.whl (793 kB)\n","\u001b[K     |████████████████████████████████| 793 kB 19.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy<1.22 in /usr/local/lib/python3.7/dist-packages (from livelossplot) (1.21.6)\n","Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot) (2.3.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot) (3.2.2)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.8.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (2.6.1)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (57.4.0)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (5.1.1)\n","Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n","  Downloading prompt_toolkit-3.0.29-py3-none-any.whl (381 kB)\n","\u001b[K     |████████████████████████████████| 381 kB 43.9 MB/s \n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (4.4.2)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.1.3)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.18.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython==7.*->livelossplot) (0.7.5)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython==7.*->livelossplot) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython==7.*->livelossplot) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.*->livelossplot) (0.2.5)\n","Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.11.3)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (3.13)\n","Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (21.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (2.8.2)\n","Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (5.1.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (4.2.0)\n","Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.8)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (1.4.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot) (0.11.0)\n","Installing collected packages: prompt-toolkit, ipython, livelossplot\n","  Attempting uninstall: prompt-toolkit\n","    Found existing installation: prompt-toolkit 1.0.18\n","    Uninstalling prompt-toolkit-1.0.18:\n","      Successfully uninstalled prompt-toolkit-1.0.18\n","  Attempting uninstall: ipython\n","    Found existing installation: ipython 5.5.0\n","    Uninstalling ipython-5.5.0:\n","      Successfully uninstalled ipython-5.5.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.29 which is incompatible.\n","google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.33.0 which is incompatible.\u001b[0m\n","Successfully installed ipython-7.33.0 livelossplot-0.5.5 prompt-toolkit-3.0.29\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["IPython","prompt_toolkit"]}}},"metadata":{}}],"source":["import os\n","try:\n","    from google.colab import drive\n","    COLAB = True\n","    print(\"Estoy en Google CoLab\")\n","    %tensorflow_version 2.x\n","    !pip install livelossplot\n","except:\n","    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n","    print(\"No estoy en Google CoLab\")\n","    COLAB = False"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"iCTvwxqzDtcM","executionInfo":{"status":"error","timestamp":1652120578605,"user_tz":-120,"elapsed":4241,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}},"outputId":"c09be097-d39a-4076-8a1c-6dcbcc1c49c3","colab":{"base_uri":"https://localhost:8080/","height":374}},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-7000254b6b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'RMSprop' from 'keras.optimizers' (/usr/local/lib/python3.7/dist-packages/keras/optimizers.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["from urllib import request\n","from keras.preprocessing.text import Tokenizer\n","import numpy as np\n","from keras.utils import np_utils\n","from keras.layers import Dense, LSTM, SimpleRNN, GRU, Input, Embedding, Dropout, Bidirectional\n","from keras.models import Model\n","from keras.callbacks import EarlyStopping\n","from keras.optimizers import RMSprop\n","from keras import layers\n","try:\n","    from livelossplot import PlotLossesKerasTF\n","except:\n","    !pip install livelossplot\n","    from livelossplot import PlotLossesKerasTF\n","    \n","import tensorflow as tf \n","import re\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MZVR30diDtcN","executionInfo":{"status":"aborted","timestamp":1652120578594,"user_tz":-120,"elapsed":15,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["def train(model, train_data, optimizer, epochs = 30, loss='mse', batch_size=32, plot=True):\n","    model.compile(\n","      optimizer=optimizer,\n","      loss=loss,\n","      metrics=['accuracy'],\n","    )\n","    callbacks = None\n","    if plot:\n","        callbacks = [PlotLossesKerasTF()]\n","    \n","    x_train, y_train = train_data\n","    model.fit(\n","      x_train, \n","      y_train,\n","      epochs=epochs,\n","      batch_size=batch_size,\n","      callbacks=callbacks,\n","      shuffle = True\n","    )\n","    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"9MV4tFKZDtcO"},"source":["## Generación de textos"]},{"cell_type":"markdown","metadata":{"id":"Cd8iAzOVDtcQ"},"source":["En este ejemplo crearemos un generador de textos. Para ello vamos a usar como corpus \"Don Quijote\" ya que tiene un lenguaje peculiar para nuestra época y nos resultará curioso ver que aprende nuestro modelo.\n","\n","### El corpus\n","\n","Podemos descargar el texto del proyecto Gutemberg:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyPOU2D0DtcR","executionInfo":{"status":"aborted","timestamp":1652120578595,"user_tz":-120,"elapsed":16,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["import requests\n","url = 'http://www.gutenberg.org/cache/epub/2000/pg2000.txt'\n","corpus = requests.get(url).content.decode(\"utf-8\").split(\"\\r\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ktk34zBQDtcS","executionInfo":{"status":"aborted","timestamp":1652120578596,"user_tz":-120,"elapsed":17,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["corpus[:10000]"]},{"cell_type":"markdown","metadata":{"id":"d0isX28XDtcU"},"source":["Lo primero que debemos hacer es limpiar el texto. En nuestro caso vamos a convertir a minúsculas todo, quitaremos los retornos de carro y los signos de puntuación:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBzbjzn_DtcV","executionInfo":{"status":"aborted","timestamp":1652120578596,"user_tz":-120,"elapsed":16,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["seq_length = 20\n","start_story = '| ' * seq_length\n","\n","text = corpus[605:-370]    # eliminamos la primeras y últimas líneas que no corresponde al libro\n","text = ' '.join(text) # unificamos en una sola cadena de texto\n","text = text.replace('\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n', start_story)\n","text = text.lower() # convertimos a minúscula\n","text = text.replace('\\r\\n', ' ') # eliminamos los retorno de carro\n","#text = re.sub('  +', '. ', text).strip() # sustituimos dos espacios o más consecutivos por un \".\"\n","text = text.replace('..', '.')\n","text = re.sub('([¡!\"#$%&()*+,-./:;<=>¿?@[\\]^_`{|}~])', r' \\1 ', text) # separamos esos símbolos del texto\n","text = re.sub('\\s{2,}', ' ', text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfjjzYniDtcW","executionInfo":{"status":"aborted","timestamp":1652120578597,"user_tz":-120,"elapsed":17,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["text[:1000]"]},{"cell_type":"markdown","metadata":{"id":"iKxDRBCkDtcX"},"source":["El siguiente paso es tokenizar, es decir particionar el texto (lo podemos particionar por token o por carácter):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lErW85WGDtcX","executionInfo":{"status":"aborted","timestamp":1652120578597,"user_tz":-120,"elapsed":17,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["tokenizer = Tokenizer(char_level = False, filters = '')\n","tokenizer.fit_on_texts([text])\n","total_words = len(tokenizer.word_index) + 1\n","token_list = tokenizer.texts_to_sequences([text])[0]"]},{"cell_type":"markdown","metadata":{"id":"u5ab1ir8DtcY"},"source":["Podemos mostrar una lista de nuestro diccionario:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6V88J3PcDtcY","executionInfo":{"status":"aborted","timestamp":1652120578598,"user_tz":-120,"elapsed":17,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["[(tokenizer.index_word[i], i) for i in token_list[:10]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G3RdpLhuDtcZ","executionInfo":{"status":"aborted","timestamp":1652120578598,"user_tz":-120,"elapsed":18,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["'Total de palabras en el vocabulario: ', len(tokenizer.index_word.keys())"]},{"cell_type":"markdown","metadata":{"id":"pf8sMCh5DtcZ"},"source":["### El conjunto de entrenamiento\n","\n","La red se entrenará para predecir el siguiente token de la secuencia dada una sencuencia previa de tokens. Por ejemplo, si le damos al modelo los tokens *era de complexión*, se podría esperar que dinera como predicción el token *recia* o *fuerte*.\n","\n","La longitud de entrada (lo tokens previos al token a predecir) es un parámetro del optimizar. En este caso usamores una longitud de 20, por lo que debemos dividir el texto en trozos de 20 palabras. Esto nos dará un conjunto de datos de 444145 (ya que el corpus completo tiene 444165 tokens)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODipr4v5Dtca","executionInfo":{"status":"aborted","timestamp":1652120578599,"user_tz":-120,"elapsed":18,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["len(token_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cefBcJPhDtca","executionInfo":{"status":"aborted","timestamp":1652120578600,"user_tz":-120,"elapsed":19,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["def generate_sequences(token_list, step):\n","    X, y = [], []\n","    \n","    for i in range(0, len(token_list) - seq_length, step):\n","        X.append(token_list[i: i + seq_length])\n","        y.append(token_list[i + seq_length])\n","        \n","    print('Total de secuencias:', len(X))\n","    \n","    return np.array(X), np.array(y)\n","\n","X, y = generate_sequences(token_list, 1)"]},{"cell_type":"markdown","metadata":{"id":"0YAL1atyDtcb"},"source":["### Primera red recurrente\n","\n","Nuestra arquitectura será una red recurrente que usará embeddings antes de las células recurrentes. En este caso dichos embeddings los tendrá que aprender la red durante el entrenamiento:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPOi_sb5Dtcb","executionInfo":{"status":"aborted","timestamp":1652120578600,"user_tz":-120,"elapsed":19,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["epochs = 20\n","batch_size = 256\n","embedding_size = 100\n","n_units = 64\n","\n","text_in = Input(shape = (None,))\n","x = text_in\n","x = Embedding(total_words, embedding_size)(x)\n","x = GRU(n_units, )(x)\n","x = Dropout(0.2)(x)\n","text_out = Dense(total_words, activation = 'softmax')(x)\n","\n","gru = Model(text_in, text_out)\n","gru.summary()              "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f8k2suDrDtcc","executionInfo":{"status":"aborted","timestamp":1652120578601,"user_tz":-120,"elapsed":19,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["train(gru, (X, y), optimizer='adam', loss='sparse_categorical_crossentropy', \n","      batch_size=batch_size, epochs=epochs)"]},{"cell_type":"markdown","metadata":{"id":"bRiwYDfeDtcd"},"source":["\n","\n","Vamos a hacer un generador de texto y vamos a unificar las partes comunes en un clase que nos ayuda a entrenar este tipo de modelos solo cambiando el model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DvL0J3fVDtcd","executionInfo":{"status":"aborted","timestamp":1652120578602,"user_tz":-120,"elapsed":20,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["class Writer:\n","    \n","    def __init__(self, next_token_model, tokenizer=None, mode='word', start_token='|', end_token='.'):\n","        self.next_token_model = next_token_model\n","        self.tokenizer = tokenizer\n","        self.mode = mode\n","        self.start_token='|'\n","        self.end_token='.'\n","\n","        \n","    def __generate_sequences(self, token_list, step=1, seq_length=20):\n","        X, y = [], []\n","    \n","        for i in range(0, len(token_list) - seq_length, step):\n","            X.append(token_list[i: i + seq_length])\n","            y.append(token_list[i + seq_length])\n","        \n","        print('Total de secuencias:', len(X))\n","    \n","        return np.array(X), np.array(y)\n","    \n","    def __train(self, train_data, optimizer='adam', epochs = 30, batch_size=32):\n","        loss = 'sparse_categorical_crossentropy'\n","        self.next_token_model.compile(\n","          optimizer=optimizer,\n","          loss=loss,\n","          metrics=['accuracy'],\n","        )\n","\n","        x_train, y_train = train_data\n","        self.next_token_model.fit(\n","          x_train, \n","          y_train,\n","          epochs=epochs,\n","          batch_size=batch_size,\n","          callbacks=[PlotLossesKeras(), EarlyStopping(patience=5,\n","                                                      restore_best_weights=True)],\n","          shuffle = True\n","        )\n","\n","\n","    def train(self, corpus, optimizer='adam', epochs = 30, batch_size=32, seq_length=20):\n","        self.tokenizer.fit_on_texts([text])\n","        total_words = len(self.tokenizer.word_index) + 1\n","        token_list = tokenizer.texts_to_sequences([text])[0]\n","        \n","        x_train, y_train = self.__generate_sequences(token_list, seq_length=seq_length)\n","        \n","        self.__train((x_train, y_train), optimizer=optimizer, \n","                     epochs=epochs, batch_size=batch_size)\n","\n","    \n","    def __sample(self, preds, temperature=1.0):\n","        preds = np.log(preds) / temperature\n","        exp_preds = np.exp(preds)\n","        preds = exp_preds / (np.sum(exp_preds)+ 1e-5)\n","        probas = np.random.multinomial(1, preds, 1)\n","        return np.argmax(probas)\n","    \n","    def write(self, start_tokens, temperature=1.0, verbose=True, max_len=10000):\n","        sentence = start_tokens\n","        if verbose:\n","            print(sentence, end='')\n","            \n","\n","        while len(sentence) < max_len:\n","            context = np.array(tokenizer.texts_to_sequences([sentence]))\n","            prob = self.next_token_model([context])\n","            \n","            index = self.__sample(prob[0,:], temperature)\n","            next_token = tokenizer.index_word[index]\n","\n","            sep = '' if self.mode=='char' else ' '\n","            if verbose:\n","                print(sep + next_token, end='')\n","            \n","            \n","            sentence += sep + next_token\n","            \n","            if next_token == self.end_token:\n","                break\n","\n","\n","        return sentence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uh3JAC6dDtce","executionInfo":{"status":"aborted","timestamp":1652120578602,"user_tz":-120,"elapsed":20,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["tf.get_logger().setLevel('INFO')\n","writer = Writer(gru, tokenizer)\n","results = writer.write('el perro estaba hambriento', temperature=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E15lnu7nDtce","executionInfo":{"status":"aborted","timestamp":1652120578603,"user_tz":-120,"elapsed":21,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["results = writer.write('el perro estaba hambriento', temperature=3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBiVHvfZDtce","executionInfo":{"status":"aborted","timestamp":1652120578603,"user_tz":-120,"elapsed":21,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["results = writer.write('el perro estaba hambriento', temperature=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AgpDQfUlDtcf","executionInfo":{"status":"aborted","timestamp":1652120578604,"user_tz":-120,"elapsed":21,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":["gru.save('gru_quijote.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sc2gXl5FDtcf","executionInfo":{"status":"aborted","timestamp":1652120578604,"user_tz":-120,"elapsed":21,"user":{"displayName":"Germán Lorenz","userId":"17546947320916964948"}}},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"2. Generación de textos.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}